{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\SED_sandbox\\DLAppRealTime\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import (\n",
    "    Audio,\n",
    "    get_dataset_split_names,\n",
    "    ClassLabel,\n",
    "    Sequence,\n",
    "    Dataset,\n",
    "    features,\n",
    "    load_dataset\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 120/120 [00:00<00:00, 97240.43it/s]\n",
      "Downloading data files: 100%|██████████| 120/120 [00:00<00:00, 21801.81it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Generating train split: 120 examples [00:00, 7997.66 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_load\n",
    "# metadata.csv, file1.wav, file2.wav....\n",
    "audio_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=\"./data/test/\",\n",
    "    # data_dir=\"./data/UrbanSound8K/audio\",\n",
    ")\n",
    "# audio_dataset = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "audio_dataset\n",
    "# audio_dataset[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/pytorch/data/test/2_clapping/3-177083-A-22.wav',\n",
       "  'array': array([ 0.02758789, -0.02697754, -0.0774231 , ...,  0.        ,\n",
       "          0.        ,  0.        ]),\n",
       "  'sampling_rate': 44100},\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train and test\n",
    "audio_dataset = audio_dataset[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/pytorch/data/test/0_pingpong/ball_racket-48.wav',\n",
       "  'array': array([0.00512695, 0.01071167, 0.00949097, ..., 0.00671387, 0.00579834,\n",
       "         0.00598145]),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[\"test\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 108/108 [00:00<00:00, 5420.55 examples/s]\n",
      "Casting the dataset: 100%|██████████| 12/12 [00:00<00:00, 641.52 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_names = [\n",
    "    \"pingpong\",\n",
    "    \"caughing\",\n",
    "    \"clapping\",\n",
    "    \"footsteps\"\n",
    "]\n",
    "class_label = ClassLabel(num_classes=len(label_names), names=label_names)\n",
    "audio_dataset = audio_dataset.cast_column(\"label\", class_label)\n",
    "audio_dataset[\"train\"].features\n",
    "audio_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2label_fn = audio_dataset[\"train\"].features[\"class\"].int2str\n",
    "id2label_fn = audio_dataset[\"train\"].features[\"label\"].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "model_id = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id,\n",
    "    do_normalize=True,\n",
    ")\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/pytorch/data/test/2_clapping/2-25292-A-22.wav',\n",
       " 'array': array([-5.48794342e-05, -4.16119758e-04, -3.39965554e-05, ...,\n",
       "        -8.52083862e-01, -6.75716877e-01,  1.30227864e-01]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sampling rate of dataset to 16k\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "audio_dataset[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 108/108 [00:00<00:00, 112.90 examples/s]\n",
      "Map: 100%|██████████| 12/12 [00:00<00:00, 135.13 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 108\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess dataset for adapting model\n",
    "# according to: https://github.com/karolpiczak/ESC-50\n",
    "# 学習する音データの長さ\n",
    "max_duration = 1\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        # return_attention_mask=False,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "audio_dataset_encoded = audio_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    # remove_columns=[\"audio\", \"classID\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "# audio_dataset_encoded = audio_dataset_encoded.rename_column(\"class\", \"label\")\n",
    "audio_dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define label and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "id2label = {str(i): id2label_fn(i) for i in range(len(audio_dataset_encoded[\"train\"].features[\"label\"].names))}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 3\n",
    "training_args = TrainingArguments(\n",
    "    f\"./model/pingpong-{model_name}-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    hub_token=\"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\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=audio_dataset_encoded[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=audio_dataset_encoded[\"test\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5/42 [00:42<05:15,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1059, 'learning_rate': 3e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/42 [01:23<04:24,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2732, 'learning_rate': 4.594594594594595e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 33%|███▎      | 14/42 [01:57<03:14,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007936358451843262, 'eval_accuracy': 1.0, 'eval_runtime': 5.1857, 'eval_samples_per_second': 2.314, 'eval_steps_per_second': 0.386, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 15/42 [02:08<04:19,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 3.918918918918919e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 20/42 [02:50<03:10,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 3.2432432432432436e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 25/42 [03:32<02:24,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 2.5675675675675675e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 28/42 [03:58<01:39,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00015245874237734824, 'eval_accuracy': 1.0, 'eval_runtime': 5.0986, 'eval_samples_per_second': 2.354, 'eval_steps_per_second': 0.392, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 30/42 [04:16<01:47,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.891891891891892e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 35/42 [04:58<01:00,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.2162162162162164e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 40/42 [05:40<00:16,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 5.405405405405406e-06, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 42/42 [05:58<00:00,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00010193387424806133, 'eval_accuracy': 1.0, 'eval_runtime': 5.1247, 'eval_samples_per_second': 2.342, 'eval_steps_per_second': 0.39, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [05:59<00:00,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 359.9354, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.117, 'train_loss': 0.2836073509284428, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42, training_loss=0.2836073509284428, metrics={'train_runtime': 359.9354, 'train_samples_per_second': 0.9, 'train_steps_per_second': 0.117, 'train_loss': 0.2836073509284428, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make convert function\n",
    "# You should make classlabel yourselve to use int2str\n",
    "# because label feature is manually added to csv column\n",
    "# label_names = [\n",
    "#     \"air_conditioner\",\n",
    "#     \"car_horn\",\n",
    "#     \"children_playing\",\n",
    "#     \"dog_bark\",\n",
    "#     \"drilling\",\n",
    "#     \"engine_idling\",\n",
    "#     \"gun_shot\",\n",
    "#     \"jackhammer\",\n",
    "#     \"siren\",\n",
    "#     \"street_music\"\n",
    "#     ]\n",
    "# label_names\n",
    "# audio_dataset = audio_dataset.cast_column(\"label\", Sequence(ClassLabel(names=label_names)))\n",
    "# # # https://github.com/huggingface/datasets/issues/5262\n",
    "# id2label_fn = audio_dataset[\"train\"].features[\"label\"].int2str()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
