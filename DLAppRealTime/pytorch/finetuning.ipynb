{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import (\n",
    "    Audio,\n",
    "    get_dataset_split_names,\n",
    "    ClassLabel,\n",
    "    Sequence,\n",
    "    Dataset,\n",
    "    features\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 8732/8732 [00:00<00:00, 18461.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/pytorch/data/test/100032-3-0-0.wav',\n",
       " 'array': array([-0.00454712, -0.00483704, -0.00460815, ..., -0.00065613,\n",
       "        -0.00048828,  0.        ]),\n",
       " 'sampling_rate': 44100}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/audio_load\n",
    "# metadata.csv, file1.wav, file2.wav....\n",
    "audio_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=\"./data/test/\",\n",
    "    # data_dir=\"./data/UrbanSound8K/audio\",\n",
    ")\n",
    "# audio_dataset = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "audio_dataset\n",
    "audio_dataset[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train and test\n",
    "audio_dataset = audio_dataset[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 7857/7857 [00:00<00:00, 19068.98 examples/s]\n",
      "Casting the dataset: 100%|██████████| 874/874 [00:00<00:00, 2446.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'class', 'classID'],\n",
       "        num_rows: 7857\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'class', 'classID'],\n",
       "        num_rows: 874\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio_dataset\n",
    "# audio_dataset[\"train\"]\n",
    "# len(audio_dataset[\"train\"][\"classID\"])\n",
    "label_names = [\n",
    "    \"air_conditioner\",\n",
    "    \"car_horn\",\n",
    "    \"children_playing\",\n",
    "    \"dog_bark\",\n",
    "    \"drilling\",\n",
    "    \"engine_idling\",\n",
    "    \"gun_shot\",\n",
    "    \"jackhammer\",\n",
    "    \"siren\",\n",
    "    \"street_music\",\n",
    "]\n",
    "# label列をClassLabelにcast\n",
    "class_label = ClassLabel(num_classes=len(label_names), names=label_names)\n",
    "audio_dataset = audio_dataset.cast_column(\"class\", class_label)\n",
    "audio_dataset[\"train\"].features\n",
    "audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_fn = audio_dataset[\"train\"].features[\"class\"].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "model_id = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id,\n",
    "    do_normalize=True,\n",
    ")\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/pytorch/data/test/105425-9-0-14.wav',\n",
       " 'array': array([-0.01866602, -0.0220116 , -0.02827694, ..., -0.10153409,\n",
       "        -0.10287194, -0.10586938]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sampling rate of dataset to 16k\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "audio_dataset[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7857/7857 [03:14<00:00, 40.47 examples/s]\n",
      "Map: 100%|██████████| 874/874 [00:21<00:00, 41.20 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 7857\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 874\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess dataset for adapting model\n",
    "# according to: https://github.com/karolpiczak/ESC-50\n",
    "max_duration = 5\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        # return_attention_mask=False,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "audio_dataset_encoded = audio_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\", \"classID\"],\n",
    "    # remove_columns=[\"audio\", \"class\"],\n",
    "    # remove_columns=[\"audio\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "audio_dataset_encoded = audio_dataset_encoded.rename_column(\"class\", \"label\")\n",
    "# audio_dataset_encoded = audio_dataset_encoded.rename_column(\"classID\", \"label\")\n",
    "# audio_dataset_encoded = audio_dataset_encoded.rename_column(\"genre\", \"label\")\n",
    "audio_dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define label and model\n",
    "\n",
    "# label_names = [\n",
    "#     \"air_conditioner\",\n",
    "#     \"car_horn\",\n",
    "#     \"children_playing\",\n",
    "#     \"dog_bark\",\n",
    "#     \"drilling\",\n",
    "#     \"engine_idling\",\n",
    "#     \"gun_shot\",\n",
    "#     \"jackhammer\",\n",
    "#     \"siren\",\n",
    "#     \"street_music\",\n",
    "# ]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# id2label = {i: label for i, label in enumerate(label_names)}\n",
    "# label2id = {v: k for k, v in id2label.items()}\n",
    "# num_labels = len(label_names)\n",
    "\n",
    "id2label = {str(i): id2label_fn(i) for i in range(len(audio_dataset_encoded[\"train\"].features[\"label\"].names))}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/565 [1:22:33<776:01:50, 4953.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# define training arguments\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 5\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-us8k\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    hub_token=\"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\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=audio_dataset_encoded[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=audio_dataset_encoded[\"test\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/4915 [00:45<12:02:28,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2615, 'learning_rate': 4.0650406504065046e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4915 [01:28<11:58:17,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4298, 'learning_rate': 9.146341463414634e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/4915 [02:12<11:57:32,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1259, 'learning_rate': 1.4227642276422764e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/4915 [02:56<11:58:04,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4506, 'learning_rate': 1.9308943089430896e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/4915 [03:40<11:57:52,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2919, 'learning_rate': 2.4390243902439027e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 30/4915 [04:24<11:55:44,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.926, 'learning_rate': 2.9471544715447155e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 35/4915 [05:08<11:56:24,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9259, 'learning_rate': 3.4552845528455287e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 40/4915 [05:53<11:57:51,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9721, 'learning_rate': 3.9634146341463414e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 45/4915 [06:37<11:56:45,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8533, 'learning_rate': 4.471544715447155e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 50/4915 [07:21<11:58:40,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6908, 'learning_rate': 4.979674796747968e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 55/4915 [08:05<11:53:16,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5733, 'learning_rate': 5.487804878048781e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 60/4915 [08:49<11:56:20,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3703, 'learning_rate': 5.995934959349594e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 65/4915 [09:33<11:51:59,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2646, 'learning_rate': 6.504065040650407e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 70/4915 [10:17<11:50:25,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9302, 'learning_rate': 7.0121951219512205e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 75/4915 [11:01<11:51:05,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0857, 'learning_rate': 7.520325203252034e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 80/4915 [11:46<11:50:53,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9369, 'learning_rate': 8.028455284552846e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 85/4915 [12:30<11:49:21,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8973, 'learning_rate': 8.53658536585366e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 90/4915 [13:14<11:51:32,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7728, 'learning_rate': 9.044715447154472e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 95/4915 [13:58<11:52:50,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7987, 'learning_rate': 9.552845528455286e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/4915 [14:42<11:48:44,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4793, 'learning_rate': 1.0060975609756099e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 105/4915 [15:26<11:46:19,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5771, 'learning_rate': 1.0569105691056911e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 110/4915 [16:10<11:45:06,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4794, 'learning_rate': 1.1077235772357725e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 115/4915 [16:54<11:44:45,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6265, 'learning_rate': 1.1585365853658537e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 120/4915 [17:38<11:42:34,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4991, 'learning_rate': 1.2093495934959351e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 125/4915 [18:22<11:43:22,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5267, 'learning_rate': 1.2601626016260162e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 130/4915 [19:06<11:41:55,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5886, 'learning_rate': 1.3109756097560976e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 135/4915 [19:51<11:43:18,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4831, 'learning_rate': 1.3617886178861788e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 140/4915 [20:35<11:41:52,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3437, 'learning_rate': 1.4126016260162602e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 145/4915 [21:19<11:41:37,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2988, 'learning_rate': 1.4634146341463415e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 150/4915 [22:03<11:37:58,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4174, 'learning_rate': 1.5142276422764229e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 155/4915 [22:47<11:37:00,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4651, 'learning_rate': 1.565040650406504e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 160/4915 [23:31<11:36:26,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4635, 'learning_rate': 1.6158536585365855e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 165/4915 [24:15<11:34:48,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3428, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 170/4915 [24:59<11:35:18,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3466, 'learning_rate': 1.717479674796748e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 175/4915 [25:43<11:35:30,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'learning_rate': 1.7682926829268292e-05, 'epoch': 0.18}\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make convert function\n",
    "# You should make classlabel yourselve to use int2str\n",
    "# because label feature is manually added to csv column\n",
    "# label_names = [\n",
    "#     \"air_conditioner\",\n",
    "#     \"car_horn\",\n",
    "#     \"children_playing\",\n",
    "#     \"dog_bark\",\n",
    "#     \"drilling\",\n",
    "#     \"engine_idling\",\n",
    "#     \"gun_shot\",\n",
    "#     \"jackhammer\",\n",
    "#     \"siren\",\n",
    "#     \"street_music\"\n",
    "#     ]\n",
    "# label_names\n",
    "# audio_dataset = audio_dataset.cast_column(\"label\", Sequence(ClassLabel(names=label_names)))\n",
    "# # # https://github.com/huggingface/datasets/issues/5262\n",
    "# id2label_fn = audio_dataset[\"train\"].features[\"label\"].int2str()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
