{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import (\n",
    "    Audio,\n",
    "    get_dataset_split_names,\n",
    "    ClassLabel,\n",
    "    Sequence,\n",
    "    Dataset,\n",
    "    features,\n",
    "    load_dataset\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 160/160 [00:00<?, ?it/s]\n",
      "Downloading data files: 100%|██████████| 160/160 [00:00<00:00, 22179.62it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Generating train split: 160 examples [00:00, 5525.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_names = [\n",
    "    \"pingpong\",\n",
    "    \"caughing\",\n",
    "    \"clapping\",\n",
    "    \"silence\",\n",
    "]\n",
    "\n",
    "# https://huggingface.co/docs/datasets/audio_load\n",
    "# metadata.csv, file1.wav, file2.wav....\n",
    "audio_dataset = load_dataset(\n",
    "    \"audiofolder\",\n",
    "    data_dir=\"./data/test/\",\n",
    "    # data_dir=\"./data/UrbanSound8K/audio\",\n",
    ")\n",
    "# audio_dataset = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "audio_dataset\n",
    "# audio_dataset[\"train\"][0][\"audio\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/data/test/2_clapping/3-177083-A-22.wav',\n",
       "  'array': array([ 0.02758789, -0.02697754, -0.0774231 , ...,  0.        ,\n",
       "          0.        ,  0.        ]),\n",
       "  'sampling_rate': 44100},\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset to train and test\n",
    "audio_dataset = audio_dataset[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/data/test/3_silence/silence-24.wav',\n",
       "  'array': array([-1.04370117e-02, -1.08032227e-02, -1.09863281e-02, ...,\n",
       "         -1.15966797e-03, -7.93457031e-04, -6.10351562e-05]),\n",
       "  'sampling_rate': 48000},\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset[\"test\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 144/144 [00:00<00:00, 7241.44 examples/s]\n",
      "Casting the dataset: 100%|██████████| 16/16 [00:00<00:00, 819.73 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class_label = ClassLabel(num_classes=len(label_names), names=label_names)\n",
    "audio_dataset = audio_dataset.cast_column(\"label\", class_label)\n",
    "audio_dataset[\"train\"].features\n",
    "audio_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2label_fn = audio_dataset[\"train\"].features[\"class\"].int2str\n",
    "id2label_fn = audio_dataset[\"train\"].features[\"label\"].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "model_id = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id,\n",
    "    do_normalize=True,\n",
    ")\n",
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'C:/GitHub/SED_sandbox/DLAppRealTime/data/test/0_pingpong/ball_racket-32.wav',\n",
       " 'array': array([ 0.00305176,  0.00405884,  0.00286865, ..., -0.00485229,\n",
       "        -0.0043335 , -0.00323486]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sampling rate of dataset to 16k\n",
    "audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "audio_dataset[\"train\"][0][\"audio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 144/144 [00:02<00:00, 63.36 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 71.25 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_values'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess dataset for adapting model\n",
    "# according to: https://github.com/karolpiczak/ESC-50\n",
    "# 学習する音データの長さ\n",
    "max_duration = 1\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        # return_attention_mask=False,\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "audio_dataset_encoded = audio_dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"audio\"],\n",
    "    # remove_columns=[\"audio\", \"classID\"],\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=1,\n",
    ")\n",
    "# audio_dataset_encoded = audio_dataset_encoded.rename_column(\"class\", \"label\")\n",
    "audio_dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define label and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "id2label = {str(i): id2label_fn(i) for i in range(len(audio_dataset_encoded[\"train\"].features[\"label\"].names))}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training arguments\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 3\n",
    "training_args = TrainingArguments(\n",
    "    f\"./model/pingpong-{model_name}-finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    hub_token=\"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\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=audio_dataset_encoded[\"train\"].with_format(\"torch\"),\n",
    "    eval_dataset=audio_dataset_encoded[\"test\"].with_format(\"torch\"),\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/54 [00:50<07:22,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5481, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 10/54 [01:32<06:11,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2194, 'learning_rate': 4.6875e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 15/54 [02:14<05:31,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 33%|███▎      | 18/54 [02:47<05:11,  8.64s/it]Checkpoint destination directory ./model/pingpong-ast-finetuned-audioset-10-10-0.4593-finetuned\\checkpoint-18 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0005850121378898621, 'eval_accuracy': 1.0, 'eval_runtime': 6.9162, 'eval_samples_per_second': 2.313, 'eval_steps_per_second': 0.289, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 20/54 [03:05<05:46, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 3.6458333333333336e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 25/54 [03:46<04:08,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 3.125e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 30/54 [04:28<03:21,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 2.604166666666667e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 35/54 [05:10<02:38,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 2.0833333333333336e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|██████▋   | 36/54 [05:24<02:29,  8.28s/it]Checkpoint destination directory ./model/pingpong-ast-finetuned-audioset-10-10-0.4593-finetuned\\checkpoint-36 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00011798739433288574, 'eval_accuracy': 1.0, 'eval_runtime': 6.8236, 'eval_samples_per_second': 2.345, 'eval_steps_per_second': 0.293, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 40/54 [05:59<02:08,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.5625e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 45/54 [06:42<01:17,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0416666666666668e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 50/54 [07:28<00:36,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 5.208333333333334e-06, 'epoch': 2.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 54/54 [08:11<00:00,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00010009109973907471, 'eval_accuracy': 1.0, 'eval_runtime': 8.1021, 'eval_samples_per_second': 1.975, 'eval_steps_per_second': 0.247, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [08:13<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 493.8597, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.109, 'train_loss': 0.1642560550460109, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=54, training_loss=0.1642560550460109, metrics={'train_runtime': 493.8597, 'train_samples_per_second': 0.875, 'train_steps_per_second': 0.109, 'train_loss': 0.1642560550460109, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
